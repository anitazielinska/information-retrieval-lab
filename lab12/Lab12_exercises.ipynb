{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOME IMPORTS\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "import multiprocessing\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET SOME ENVIRONMENTAL VARIABLES\n",
    "os.environ['PYSPARK_PYTHON']=\"python\"\n",
    "os.environ['SPARK_LOCAL_HOSTNAME']=\"localhost\"\n",
    "os.environ['SPARK_HOME']=\"C:\\\\Users\\\\krajz\\\\Desktop\\\\projekty\\\\information-retrieval-lab\\\\lab12\\\\spark-2.2.1-bin-hadoop2.7\"\n",
    "os.environ['JAVA_HOME']=\"C:\\\\Java2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in d:\\programy\\anaconda\\lib\\site-packages (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECK IF FINDSPARK WORKS CORRECTLY\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# START SPARK CONTEXT ON LOCAL MACHINE\n",
    "sc = SparkContext(\"local\", appName=\"Test\")\n",
    "##------------------------------------\n",
    "# GO TO LOCALHOST:4040 and ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STOP SPARK CONTEXT\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of logical CPUs is 12\n"
     ]
    }
   ],
   "source": [
    "# OBTAIN THE NUMBER OF LOGICAL CPUs\n",
    "cpus = multiprocessing.cpu_count()\n",
    "print(\"The number of logical CPUs is \" + str(cpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: Compute the value of PI using Monte Carlo Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This exercise is solved. Your task is to read and analyse the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this method generates one sample point and verifies whether it is inside a circle or not.\n",
    "# The input is passed via filter method, however, we do not need it here\n",
    "def inside(inValue):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x*x + y*y < 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method estimates the value of PI\n",
    "def computePI_MonteCarlo_v1(sc, samples, partitions):\n",
    "    # Create Resilient Distributed Dataset (RDDs) containing SAMPLES elements.\n",
    "    # This data is distributed (parallelized) among available nodes (here, CPUs - partitions).\n",
    "    dff = sc.parallelize(range(0, samples), partitions)\n",
    "    # Filter out these samples that are not inside a circle.\n",
    "    # For this purpose, Inside method is run and returns\n",
    "    # true/false (for each data element) with appropriate probability distribution\n",
    "    # Why do we generate samples \"on fly\"?\n",
    "    filtered = dff.filter(inside)\n",
    "    # count the number of hits\n",
    "    left = filtered.count()\n",
    "    # Estimate the value of PI and return it\n",
    "    return 4.0 * float(left) / float(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo simulation for 10000000 samples\n",
      "True value of PI = 3.1415926535...\n",
      "  Number of CPUs = 1 | Time = 7.1166 s | Result(PI) = 3.14112840\n",
      "  Number of CPUs = 2 | Time = 5.0399 s | Result(PI) = 3.14150520\n",
      "  Number of CPUs = 3 | Time = 4.8651 s | Result(PI) = 3.14218040\n",
      "  Number of CPUs = 4 | Time = 5.0862 s | Result(PI) = 3.14139360\n",
      "  Number of CPUs = 5 | Time = 5.3461 s | Result(PI) = 3.14191280\n",
      "  Number of CPUs = 6 | Time = 5.8074 s | Result(PI) = 3.14215920\n",
      "  Number of CPUs = 7 | Time = 6.0923 s | Result(PI) = 3.14164320\n",
      "  Number of CPUs = 8 | Time = 6.6374 s | Result(PI) = 3.14136240\n",
      "  Number of CPUs = 9 | Time = 6.8529 s | Result(PI) = 3.14117800\n",
      "  Number of CPUs = 10 | Time = 7.4313 s | Result(PI) = 3.14152560\n",
      "  Number of CPUs = 11 | Time = 7.9094 s | Result(PI) = 3.14318960\n",
      "  Number of CPUs = 12 | Time = 8.7570 s | Result(PI) = 3.14190000\n"
     ]
    }
   ],
   "source": [
    "### ESTIMATE VALUE OF PI \n",
    "samples = 10000000\n",
    "\n",
    "print(\"Monte Carlo simulation for \" + str(samples) + \" samples\")\n",
    "print(\"True value of PI = 3.1415926535...\")\n",
    "\n",
    "## i = number of nodes (CPUs)\n",
    "for i in range(1, cpus + 1):\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"PI_MonteCarlo\")\n",
    "    start_time = time.time()\n",
    "    piValue = computePI_MonteCarlo_v1(sc, samples, i)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"  Number of CPUs = %i | Time = %.4f s | Result(PI) = %.8f\" % (i, elapsed, piValue))  \n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise 2: Wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy collection 1: 3 short documents\n",
    "# create RDD divided into n-paritions\n",
    "def getSmallCollection_EX1(sc, partitions):\n",
    "    doc1 = \"Roses,are red \"\n",
    "    doc2 = \"Roses are roses\"\n",
    "    doc3 = \"The Sun is red.\"\n",
    "    rdd1 = sc.parallelize([doc1, doc2, doc3], partitions)\n",
    "    return rdd1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Dummy collection 2: ~200 documents about animals (ant.html, dog.html, panda.html, hedgehog.html, etc.). For this purpose, download www.cs.put.poznan.pl/mtomczyk/ir/lab6/pages.zip, unzip, and copy \"pages\" folder into your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargeCollection_EX1(sc, partitions):\n",
    "    # Musialem ladowac pliki manualnie bo cos nie dzialalo\n",
    "    listFiles = os.listdir(\"./pages/\")\n",
    "    table = []\n",
    "    for i in listFiles:\n",
    "        with open(\"./pages/\" + i) as myFil:\n",
    "            table.append((i, myFil.read()))\n",
    "    DOCS = sc.parallelize(table, partitions)\n",
    "    #DOCS = sc.wholeTextFiles(\"./pages/\", partitions)\n",
    "    rdd1 = DOCS.map(lambda x: x[1])\n",
    "    return rdd1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For a given text \"x\", this method performs simple tokenization and normalization (returns a list of terms)\n",
    "def tokenizeAndNormalize(x):\n",
    "    return [s.lower() for s in re.split(' |;|,|\\t|\\n|\\.', x) if len(s) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Init spark context (1 core):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local[1]\", appName=\"Word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) DONE: Collect the data (getSmallCollection_EX1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Roses,are red ', 'Roses are roses', 'The Sun is red.']\n"
     ]
    }
   ],
   "source": [
    "rdd1 = getSmallCollection_EX1(sc, 1)\n",
    "# if you whish to print data stored in rdd, use print(rdd.collect())\n",
    "print(rdd1.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) DONE: Firslty, you should tokenize all documents. For this purpose use flatMap function (rdd2 = rdd1.flatMap) where you pass tokenizeAndNormalize method. There are two methods: map and flatMap. Both produce an output for each element of RDD object. The difference is that map keeps produced elements organised and flatMap puts them into a single list, e.g.: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 2), ('b', 3)]\n",
      "['a', 2, 'b', 3]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"b\", 2)])\n",
    "print(tempRDD.map(lambda x: (x[0], x[1]+1)).collect())\n",
    "print(tempRDD.flatMap(lambda x: (x[0], x[1]+1)).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['roses', 'are', 'red', 'roses', 'are', 'roses', 'the', 'sun', 'is', 'red']\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here (flatMap with tokenizeAndNormalize):\n",
    "rdd2 = rdd1.flatMap(tokenizeAndNormalize)\n",
    "print(rdd2.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) DONE: Now for each term produce (term, 1). Use map (why not flatMap?) with lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 1), ('are', 1), ('red', 1), ('roses', 1), ('are', 1), ('roses', 1), ('the', 1), ('sun', 1), ('is', 1), ('red', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd3 = rdd2.map(lambda x: (x, 1))\n",
    "print(rdd3.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) DONE: Now it is time to group the results. Use groupByKey method. When any \"...byKey\" method is invoked, the first element of a stored object is treated as a key. When invoking this method, you should also invoke .mapValues(list) so that all corresponding values will be stored in a single list. E.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', [1, 1])]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"a\", 1)])\n",
    "print(tempRDD.groupByKey().mapValues(list).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', [1, 1, 1]), ('are', [1, 1]), ('red', [1, 1]), ('the', [1]), ('sun', [1]), ('is', [1])]\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here:\n",
    "rdd4 = rdd3.groupByKey().mapValues(list)\n",
    "print(rdd4.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) DONE: Now you could use countByKey method but it returns a dictionarty. Use map function again to sum the elements of a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 3), ('are', 2), ('red', 2), ('the', 1), ('sun', 1), ('is', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd5 = rdd4.map(lambda x: (x[0], sum(x[1])))\n",
    "print(rdd5.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8) DONE: It is almost done but we wish the objects to be sorted (alphabetically). You can use sortByKey method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 2), ('is', 1), ('red', 2), ('roses', 3), ('sun', 1), ('the', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd6 = rdd5.sortByKey()\n",
    "print(rdd6.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9) DONE: Done. Bout it could be done in another way. Instead of grouping by key (rdd4) and counting the number of \"1\"s (rdd5), you could use reduceByKey method. reduceByKey \"merges\" all object with the same key. Similar to groupByKey, however, instead of grouping, a new value is computed by provided function, e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 4), ('b', 2)]\n"
     ]
    }
   ],
   "source": [
    "tempRDD = sc.parallelize([(\"a\", 1), (\"b\", 2), (\"a\", 3)])\n",
    "print(tempRDD.reduceByKey(lambda x, y: x + y).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('roses', 3), ('are', 2), ('red', 2), ('the', 1), ('sun', 1), ('is', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Complete the task here. Use rdd3 object to compute rdd7.\n",
    "rdd7 = rdd3.reduceByKey(lambda x, y: x + y)\n",
    "print(rdd7.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10) DONE: Sort the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('are', 2), ('is', 1), ('red', 2), ('roses', 3), ('sun', 1), ('the', 1)]\n"
     ]
    }
   ],
   "source": [
    "rdd8 = rdd7.sortByKey()\n",
    "print(rdd8.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11) DONE: Complete the method doWordCount (just copy your code, use groupByKey + map(sum) version; should return last rdd object):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doWordCount(sc, collection, partitions):\n",
    "    rdd1 = collection\n",
    "    rdd2 = rdd1.flatMap(tokenizeAndNormalize)\n",
    "    rdd3 = rdd2.map(lambda x: (x, 1))\n",
    "    rdd4 = rdd3.groupByKey().mapValues(list)\n",
    "    rdd5 = rdd4.map(lambda x: (x[0], sum(x[1])))\n",
    "    rdd6 = rdd5.sortByKey()\n",
    "    return rdd6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) DONE: Run the script and observe the results (why is the best time for 1CPU?):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 0.0190 s \n",
      "Number of CPUs = 2 | Time = 7.2227 s \n",
      "Number of CPUs = 3 | Time = 8.7664 s \n",
      "Number of CPUs = 4 | Time = 10.1744 s \n",
      "Number of CPUs = 5 | Time = 11.3842 s \n",
      "Number of CPUs = 6 | Time = 13.3190 s \n",
      "Number of CPUs = 7 | Time = 14.6375 s \n",
      "Number of CPUs = 8 | Time = 16.1300 s \n",
      "Number of CPUs = 9 | Time = 17.5880 s \n",
      "Number of CPUs = 10 | Time = 19.1373 s \n",
      "Number of CPUs = 11 | Time = 20.4006 s \n",
      "Number of CPUs = 12 | Time = 22.1965 s \n"
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "sc.stop()\n",
    "for i in range(1, cpus + 1):\n",
    "    master = \"local[\"+str(i)+\"]\"\n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getSmallCollection_EX1(sc, i)\n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13) DONE: Modyfy the above script (work on a copy, use the cell below) so that the top 3 most common words are printed. Use 1-2CPUs. computedData is an RDD object so you can use sortBy function to resort the elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 0.0190 s \n",
      "   0 : 'roses' occured 3 times\n",
      "   1 : 'are' occured 2 times\n",
      "   2 : 'red' occured 2 times\n",
      "Number of CPUs = 2 | Time = 14.0864 s \n",
      "   0 : 'roses' occured 3 times\n",
      "   1 : 'are' occured 2 times\n",
      "   2 : 'red' occured 2 times\n"
     ]
    }
   ],
   "source": [
    "# do the task here\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getSmallCollection_EX1(sc, i)\n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 3): #print top 3\n",
    "        print(\"   %i : '%s' occured %d times\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14) TODO: Repeat the experiment for 1-2CPUs and for 2nd collection (much larger). Compare computation times and print the top 20 most common words. Are the results (the most frequent words) similar to the list of english stop words? Why is the difference in time not as big as in \"PI\" example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 0.0465 s \n",
      "   0 : 'the' occured 3027 times\n",
      "   1 : 'and' occured 1910 times\n",
      "   2 : 'of' occured 1553 times\n",
      "   3 : 'in' occured 1165 times\n",
      "   4 : 'are' occured 1031 times\n",
      "   5 : 'to' occured 962 times\n",
      "   6 : 'a' occured 769 times\n",
      "   7 : 'is' occured 622 times\n",
      "   8 : 'as' occured 560 times\n",
      "   9 : 'species' occured 558 times\n",
      "   10 : 'they' occured 370 times\n",
      "   11 : 'for' occured 362 times\n",
      "   12 : 'with' occured 352 times\n",
      "   13 : 'have' occured 344 times\n",
      "   14 : 'their' occured 326 times\n",
      "   15 : 'or' occured 306 times\n",
      "   16 : 'from' occured 269 times\n",
      "   17 : 'by' occured 244 times\n",
      "   18 : 'on' occured 230 times\n",
      "   19 : 'which' occured 214 times\n",
      "Number of CPUs = 2 | Time = 14.1438 s \n",
      "   0 : 'the' occured 3027 times\n",
      "   1 : 'and' occured 1910 times\n",
      "   2 : 'of' occured 1553 times\n",
      "   3 : 'in' occured 1165 times\n",
      "   4 : 'are' occured 1031 times\n",
      "   5 : 'to' occured 962 times\n",
      "   6 : 'a' occured 769 times\n",
      "   7 : 'is' occured 622 times\n",
      "   8 : 'as' occured 560 times\n",
      "   9 : 'species' occured 558 times\n",
      "   10 : 'they' occured 370 times\n",
      "   11 : 'for' occured 362 times\n",
      "   12 : 'with' occured 352 times\n",
      "   13 : 'have' occured 344 times\n",
      "   14 : 'their' occured 326 times\n",
      "   15 : 'or' occured 306 times\n",
      "   16 : 'from' occured 269 times\n",
      "   17 : 'by' occured 244 times\n",
      "   18 : 'on' occured 230 times\n",
      "   19 : 'which' occured 214 times\n"
     ]
    }
   ],
   "source": [
    "# do the task here\n",
    "sc.stop()\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\"\n",
    "    sc = SparkContext(master, appName=\"WordCount\")\n",
    "    start_time = time.time()\n",
    "    rdd1 = getLargeCollection_EX1(sc, i)\n",
    "    computedData = doWordCount(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for j in range(0, 20):\n",
    "        print(\"   %i : '%s' occured %d times\" % (j, sortedData[j][0], sortedData[j][1]))\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Inverted Index + Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you are asked to construct inverted index in the following form: (term, the number of doccuments in which the term occurs , sorted list of docIDs]. For instance: [...,(\"roses\", 2, [0, 1]),...] -> term \"roses\" occurs in two documents: termIDs = 0 and 1. The \"get...Collection\" methods are slightly modified. Both return: rdd object, list of the names of the documents, and a dictionary (docID -> document name):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSmallCollection_EX2(sc, partitions):\n",
    "    doc1 = \"Roses,are red \"\n",
    "    doc2 = \"Roses are roses\"\n",
    "    doc3 = \"The Sun in red.\"\n",
    "    rdd1 = sc.parallelize([doc1, doc2, doc3], partitions)\n",
    "    docNames = [\"doc1\", \"doc2\", \"doc3\"]\n",
    "    docIDs = {0: docNames[0], 1: docNames[1], 2: docNames[2]}\n",
    "    return rdd1, docNames, docIDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLargeCollection_EX2(sc, partitions):\n",
    "    listFiles = os.listdir(\"./pages/\")\n",
    "    table = []\n",
    "    for i in listFiles:\n",
    "        with open(\"./pages/\" + i) as myFil:\n",
    "            table.append((i, myFil.read()))\n",
    "    DOCS = sc.parallelize(table, partitions)\n",
    "    #DOCS = sc.wholeTextFiles(\"./pages/\", partitions)\n",
    "    rdd1 = DOCS.map(lambda x: x[1])\n",
    "    rdd2 = DOCS.map(lambda x: x[0])\n",
    "    docNames = rdd2.collect()\n",
    "    docIDs = [i for i in range(0, len(docNames))]\n",
    "    return rdd1, docNames, docIDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: do the task and verify the results using the small collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeAndNormalize2(x):\n",
    "    aList = [s.lower() for s in re.split(' |;|,|\\t|\\n|\\.', x) if len(s) > 0]\n",
    "    return aList\n",
    "\n",
    "def doInvertedIndex(sc, collection, partitions):\n",
    "    rddM = list(enumerate(collection.map(tokenizeAndNormalize).collect()))\n",
    "    rdd1 = collection.flatMap(tokenizeAndNormalize)\n",
    "    rdd2 = rdd1.map(lambda x: (x, 1))\n",
    "    rdd3 = rdd2.groupByKey().mapValues(list)\n",
    "    rdd4 = rdd3.map(lambda x: (x[0], sum(x[1])))\n",
    "    rdd5 = rdd4.map(lambda x: (x[0], x[1], [i[0] for i in rddM if x[0] in i[1]]))\n",
    "    return rdd5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Run the following script and verify the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 2.5561 s \n",
      "   0 : 'roses' occured in 3 documents\n",
      "   1 : 'are' occured in 2 documents\n",
      "   2 : 'red' occured in 2 documents\n",
      "   3 : 'the' occured in 1 documents\n",
      "   4 : 'sun' occured in 1 documents\n",
      "Number of CPUs = 2 | Time = 10.0993 s \n",
      "   0 : 'roses' occured in 3 documents\n",
      "   1 : 'are' occured in 2 documents\n",
      "   2 : 'red' occured in 2 documents\n",
      "   3 : 'sun' occured in 1 documents\n",
      "   4 : 'in' occured in 1 documents\n"
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "#Why the best time is for 1CPU???\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"InvertedIndex\")\n",
    "    start_time = time.time()\n",
    "    rdd1, docNames, docIDs = getSmallCollection_EX2(sc, i)\n",
    "    computedData = doInvertedIndex(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 5): #print top 3\n",
    "        print(\"   %i : '%s' occured in %i documents\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12) Run the following script and verify if it is faster for 2 cores. Lastly, compare the obtained results with the results of exercise 2 (word count). Are the rankings corellated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPUs = 1 | Time = 5.1372 s \n",
      "   0 : 'the' occured in 3027 documents\n",
      "   1 : 'and' occured in 1910 documents\n",
      "   2 : 'of' occured in 1553 documents\n",
      "   3 : 'in' occured in 1165 documents\n",
      "   4 : 'are' occured in 1031 documents\n",
      "   5 : 'to' occured in 962 documents\n",
      "   6 : 'a' occured in 769 documents\n",
      "   7 : 'is' occured in 622 documents\n",
      "   8 : 'as' occured in 560 documents\n",
      "   9 : 'species' occured in 558 documents\n",
      "   10 : 'they' occured in 370 documents\n",
      "   11 : 'for' occured in 362 documents\n",
      "   12 : 'with' occured in 352 documents\n",
      "   13 : 'have' occured in 344 documents\n",
      "   14 : 'their' occured in 326 documents\n",
      "   15 : 'or' occured in 306 documents\n",
      "   16 : 'from' occured in 269 documents\n",
      "   17 : 'by' occured in 244 documents\n",
      "   18 : 'on' occured in 230 documents\n",
      "   19 : 'which' occured in 214 documents\n",
      "Number of CPUs = 2 | Time = 17.4069 s \n",
      "   0 : 'the' occured in 3027 documents\n",
      "   1 : 'and' occured in 1910 documents\n",
      "   2 : 'of' occured in 1553 documents\n",
      "   3 : 'in' occured in 1165 documents\n",
      "   4 : 'are' occured in 1031 documents\n",
      "   5 : 'to' occured in 962 documents\n",
      "   6 : 'a' occured in 769 documents\n",
      "   7 : 'is' occured in 622 documents\n",
      "   8 : 'as' occured in 560 documents\n",
      "   9 : 'species' occured in 558 documents\n",
      "   10 : 'they' occured in 370 documents\n",
      "   11 : 'for' occured in 362 documents\n",
      "   12 : 'with' occured in 352 documents\n",
      "   13 : 'have' occured in 344 documents\n",
      "   14 : 'their' occured in 326 documents\n",
      "   15 : 'or' occured in 306 documents\n",
      "   16 : 'from' occured in 269 documents\n",
      "   17 : 'by' occured in 244 documents\n",
      "   18 : 'on' occured in 230 documents\n",
      "   19 : 'which' occured in 214 documents\n"
     ]
    }
   ],
   "source": [
    "## i = number of nodes (CPUs). \n",
    "#Why the best time is for 1CPU???\n",
    "sc.stop()\n",
    "for i in [1,2]:\n",
    "    master = \"local[\"+str(i)+\"]\" \n",
    "    sc = SparkContext(master, appName=\"InvertedIndex\")\n",
    "    start_time = time.time()\n",
    "    rdd1, docNames, docIDs = getLargeCollection_EX2(sc, i)\n",
    "    computedData = doInvertedIndex(sc, rdd1, i)\n",
    "    rddSort = computedData.sortBy(lambda x: -x[1])\n",
    "    elapsed = time.time() - start_time\n",
    "    print(\"Number of CPUs = %i | Time = %.4f s \" % (i, elapsed))  \n",
    "    ### PRINT HERE \n",
    "    sortedData = rddSort.collect()\n",
    "    for i in range(0, 20): #print top 3\n",
    "        print(\"   %i : '%s' occured in %i documents\" % (i, sortedData[i][0], sortedData[i][1]))\n",
    "    ###\n",
    "    sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
